import { Injectable } from '@nestjs/common'
import { ConfigService } from '@nestjs/config'
import { ChatOpenAI, OpenAIEmbeddings } from '@langchain/openai'
import { z } from 'zod'
import { tool } from 'langchain'
import { Chroma } from '@langchain/community/vectorstores/chroma'
import { Observable } from 'rxjs'

import { TextLoader } from '@langchain/classic/document_loaders/fs/text'
import { RecursiveCharacterTextSplitter } from '@langchain/classic/text_splitter'

export enum AIProvider {
	DEEPSEEK = 'deepseek',
	// XIAOAI = 'xiaoai',
	BAILIAN = 'bailian'
}

@Injectable()
export class LangchainService {
	private modelInstances: Map<AIProvider, ChatOpenAI> = new Map()

	constructor(private readonly configService: ConfigService) {
		this.initializeModels()
	}

	// ===================== åˆå§‹åŒ–æ–‡æ¡£æ•°æ® =====================
	async initDocument1() {
		const loader = new TextLoader('./src/modules/langchain/document_1.txt')
		const docs = await loader.load()

		const splitter = new RecursiveCharacterTextSplitter({
			chunkSize: 400,
			chunkOverlap: 0
		})
		const chunks = await splitter.splitDocuments(docs)

		const embeddings = new OpenAIEmbeddings({
			model: 'text-embedding-v4',
			apiKey: this.configService.get('BAILIAN_API_KEY'),
			configuration: {
				baseURL: this.configService.get('BAILIAN_BASE_URL')
			},
			batchSize: 10 // è®¾ç½®æ‰¹é‡å¤§å°ä¸º10ï¼Œç¬¦åˆé˜¿é‡Œäº‘APIé™åˆ¶
		})

		const vectorStore = new Chroma(embeddings, {
			collectionName: 'document_1'
		})
		const res = await vectorStore.addDocuments(chunks)
		return res
	}

	// ===================== æ ¹æ®æ–‡æ¡£ä¸­çš„å†…å®¹è¿”å›ç­”æ¡ˆ =====================
	async getDocumentAnswer(question: string) {
		const model = new ChatOpenAI({
			model: 'qwen3-max',
			apiKey: this.configService.get('BAILIAN_API_KEY'),
			temperature: 0,
			configuration: {
				baseURL: this.configService.get('BAILIAN_BASE_URL')
			}
		})

		const embeddings = new OpenAIEmbeddings({
			model: 'text-embedding-v4',
			apiKey: this.configService.get('BAILIAN_API_KEY'),
			configuration: {
				baseURL: this.configService.get('BAILIAN_BASE_URL')
			}
		})

		const vectorStore = new Chroma(embeddings, {
			collectionName: 'document_1'
		})

		// ç”Ÿæˆé—®é¢˜å˜ä½“
		const prompt = `ä½ æ˜¯ä¸€ä¸ª AI è¯­è¨€æ¨¡å‹åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸ºç»™å®šçš„ç”¨æˆ·é—®é¢˜ç”Ÿæˆ 3 ä¸ªä¸åŒç‰ˆæœ¬çš„å˜ä½“ï¼Œä»¥ä¾¿ä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ã€‚
						é€šè¿‡ä»å¤šä¸ªè§’åº¦ç”Ÿæˆç”¨æˆ·é—®é¢˜ï¼Œä½ çš„ç›®æ ‡æ˜¯å¸®åŠ©ç”¨æˆ·å…‹æœåŸºäºè·ç¦»çš„ç›¸ä¼¼æ€§æœç´¢çš„ä¸€äº›å±€é™æ€§ã€‚
						è¯·åªè¿”å›ç”Ÿæˆçš„é—®é¢˜ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ã€‚
						åŸå§‹é—®é¢˜: ${question}`

		const variationResponse = await model.invoke(prompt)

		const { content } = variationResponse

		const variations = content
			.toString()
			.split('\n')
			.map((v) => v.trim())
			.filter((v) => v.length > 0)

		// å°†åŸå§‹é—®é¢˜å’Œå˜ä½“é—®é¢˜ä¸€èµ·è¿›è¡Œæ£€ç´¢
		const query = [question, ...variations]

		// å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æœç´¢
		const searchPromises = query.map((q) =>
			vectorStore.similaritySearch(q, 2)
		)
		const results = await Promise.all(searchPromises)

		// ç»“æœå»é‡
		const uniqueDocsMap = new Map()
		results.flat().forEach((doc) => {
			if (!uniqueDocsMap.has(doc.pageContent)) {
				uniqueDocsMap.set(doc.pageContent, doc)
			}
		})

		const uniqueDocs = Array.from(uniqueDocsMap.values())

		const context = uniqueDocs.map((doc) => doc.pageContent).join('\n\n')

		const promptText = `æ ¹æ®ä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡å†…å®¹ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·å›ç­”â€œæ— æ³•ä»æä¾›çš„å†…å®¹ä¸­æ‰¾åˆ°ç­”æ¡ˆã€‚â€ã€‚
							ä¸Šä¸‹æ–‡:
							${context}

							é—®é¢˜ï¼š
							${question}

							ç­”æ¡ˆ:
					`

		const response = await model.invoke(promptText)

		const result = response.content.toString().trim()

		return result
	}

	// ===================== ä½¿ç”¨ Observable æ¨¡å¼æµå¼è¿”å›æ–‡æ¡£ç­”æ¡ˆ =====================
	getDocumentAnswerObservable(question: string): Observable<any> {
		return new Observable((subscriber) => {
			this.processDocumentAnswerStream(question, subscriber)
		})
	}

	/**
	 * å¤„ç†æ–‡æ¡£ç­”æ¡ˆæµçš„æ ¸å¿ƒé€»è¾‘
	 */
	private async processDocumentAnswerStream(question: string, subscriber: any) {
		try {
			// 1. è¾“å‡ºå¼€å§‹å¤„ç†çš„çŠ¶æ€
			subscriber.next({
				type: 'status',
				message: 'ğŸ”„ å¼€å§‹å¤„ç†é—®é¢˜...',
				step: 'start'
			})

			const model = new ChatOpenAI({
				model: 'qwen3-max',
				apiKey: this.configService.get('BAILIAN_API_KEY'),
				temperature: 0,
				configuration: {
					baseURL: this.configService.get('BAILIAN_BASE_URL')
				}
			})

			const embeddings = new OpenAIEmbeddings({
				model: 'text-embedding-v4',
				apiKey: this.configService.get('BAILIAN_API_KEY'),
				configuration: {
					baseURL: this.configService.get('BAILIAN_BASE_URL')
				}
			})

			const vectorStore = new Chroma(embeddings, {
				collectionName: 'document_1'
			})

			// 2. ç”Ÿæˆé—®é¢˜å˜ä½“
			subscriber.next({
				type: 'status',
				message: 'ğŸ¤” æ­£åœ¨ç”Ÿæˆé—®é¢˜å˜ä½“...',
				step: 'generate_variations'
			})

			const prompt = `ä½ æ˜¯ä¸€ä¸ª AI è¯­è¨€æ¨¡å‹åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸ºç»™å®šçš„ç”¨æˆ·é—®é¢˜ç”Ÿæˆ 3 ä¸ªä¸åŒç‰ˆæœ¬çš„å˜ä½“,ä»¥ä¾¿ä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ã€‚
							é€šè¿‡ä»å¤šä¸ªè§’åº¦ç”Ÿæˆç”¨æˆ·é—®é¢˜,ä½ çš„ç›®æ ‡æ˜¯å¸®åŠ©ç”¨æˆ·å…‹æœåŸºäºè·ç¦»çš„ç›¸ä¼¼æ€§æœç´¢çš„ä¸€äº›å±€é™æ€§ã€‚
							è¯·åªè¿”å›ç”Ÿæˆçš„é—®é¢˜,æ¯è¡Œä¸€ä¸ª,ä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ã€‚
							åŸå§‹é—®é¢˜: ${question}`

			const variationResponse = await model.invoke(prompt)
			const { content } = variationResponse

			const variations = content
				.toString()
				.split('\n')
				.map((v) => v.trim())
				.filter((v) => v.length > 0)

			subscriber.next({
				type: 'status',
				message: `âœ… å·²ç”Ÿæˆ ${variations.length} ä¸ªé—®é¢˜å˜ä½“`,
				step: 'variations_generated',
				data: { variations }
			})

			// 3. æ£€ç´¢ç›¸å…³æ–‡æ¡£
			subscriber.next({
				type: 'status',
				message: 'ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£...',
				step: 'search_documents'
			})

			const query = [question, ...variations]
			const searchPromises = query.map((q) =>
				vectorStore.similaritySearch(q, 2)
			)
			const results = await Promise.all(searchPromises)

			// ç»“æœå»é‡
			const uniqueDocsMap = new Map()
			results.flat().forEach((doc) => {
				if (!uniqueDocsMap.has(doc.pageContent)) {
					uniqueDocsMap.set(doc.pageContent, doc)
				}
			})

			const uniqueDocs = Array.from(uniqueDocsMap.values())

			subscriber.next({
				type: 'status',
				message: `ğŸ“„ æ‰¾åˆ° ${uniqueDocs.length} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ`,
				step: 'documents_found',
				data: { documentCount: uniqueDocs.length }
			})

			// 4. æ„å»ºä¸Šä¸‹æ–‡å¹¶ç”Ÿæˆç­”æ¡ˆ
			subscriber.next({
				type: 'status',
				message: 'ğŸ’­ æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...',
				step: 'generate_answer'
			})

			const context = uniqueDocs.map((doc) => doc.pageContent).join('\n\n')

			const promptText = `æ ¹æ®ä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡å†…å®¹,å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯,è¯·å›ç­”"æ— æ³•ä»æä¾›çš„å†…å®¹ä¸­æ‰¾åˆ°ç­”æ¡ˆã€‚"ã€‚
								ä¸Šä¸‹æ–‡:
								${context}

								é—®é¢˜ï¼š
								${question}

								ç­”æ¡ˆ:
						`

		// 5. æµå¼è¾“å‡ºæœ€ç»ˆç­”æ¡ˆ
		subscriber.next({
			type: 'answer_start',
			message: 'ğŸ“ å¼€å§‹è¾“å‡ºç­”æ¡ˆ',
			step: 'answer_streaming'
		})

		const stream = await model.stream(promptText)

		for await (const chunk of stream) {
			// ç›´æ¥å‘é€åŸå§‹çš„ chunk.content,ä¿æŒ LangChain çš„åŸå§‹æµå¼è¾“å‡ºç²’åº¦
			subscriber.next({
				type: 'answer',
				content: chunk.content
			})
			}
			
			// 6. å®Œæˆ
			subscriber.next({
				type: 'status',
				message: 'âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆ',
				step: 'completed'
			})

			subscriber.complete()
		} catch (error) {
			subscriber.error(error)
		}
	}

	/**
	 * åˆå§‹åŒ–æ‰€æœ‰AIæ¨¡å‹å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
	 */
	private initializeModels() {
		// Deepseek
		this.modelInstances.set(
			AIProvider.DEEPSEEK,
			new ChatOpenAI({
				model: 'deepseek-chat',
				apiKey: this.configService.get<string>('DEEPSEEK_API_KEY'),
				configuration: {
					baseURL: this.configService.get<string>('DEEPSEEK_BASE_URL')
				}
			})
		)

		// å°çˆ± AI
		// this.modelInstances.set(
		// 	AIProvider.XIAOAI,
		// 	new ChatOpenAI({
		// 		model: 'gpt-4o-mini',
		// 		apiKey: this.configService.get<string>('XIAOAI_API_KEY'),
		// 		configuration: {
		// 			baseURL: this.configService.get<string>('XIAOAI_BASE_URL')
		// 		}
		// 	})
		// );

		// ç™¾ç‚¼
		this.modelInstances.set(
			AIProvider.BAILIAN,
			new ChatOpenAI({
				model: 'qwen-plus',
				apiKey: this.configService.get<string>('BAILIAN_API_KEY'),
				configuration: {
					baseURL: this.configService.get<string>('BAILIAN_BASE_URL')
				}
			})
		)
	}

	/**
	 * è·å–æŒ‡å®šä¾›åº”å•†çš„æ¨¡å‹å®ä¾‹
	 * @param provider AIä¾›åº”å•†
	 * @returns ChatOpenAIå®ä¾‹
	 */
	getChatModel(provider: AIProvider = AIProvider.DEEPSEEK): ChatOpenAI {
		const model = this.modelInstances.get(provider)
		if (!model) {
			throw new Error(`AI provider ${provider} not found`)
		}
		return model
	}

	/**
	 * è·å–æµå¼å“åº”ä»¥æ”¯æŒ SSE æ¨é€
	 */
	async streamChatResponse(
		question: string,
		provider: AIProvider = AIProvider.DEEPSEEK
	): Promise<AsyncIterable<unknown>> {
		const model = this.getChatModel(provider)
		return model.stream(question)
	}

	/**
	 * è·å–æ‰€æœ‰å¯ç”¨çš„ä¾›åº”å•†
	 */
	getAvailableProviders(): AIProvider[] {
		return Array.from(this.modelInstances.keys())
	}

	async invokeWithTools(
		provider: AIProvider = AIProvider.DEEPSEEK
	): Promise<any> {
		// 1. å®šä¹‰ä¸€ä¸ªç®€å•çš„å·¥å…·ï¼ˆä¾‹å¦‚ï¼šåŠ æ³•è®¡ç®—å™¨ï¼‰
		const calculatorSchema = z.object({
			a: z.number().describe('ç¬¬ä¸€ä¸ªæ•°å­—'),
			b: z.number().describe('ç¬¬äºŒä¸ªæ•°å­—')
		})

		const calculatorTool = tool(
			async ({ a, b }) => {
				return `${a + b}`
			},
			{
				name: 'calculator',
				description: 'è®¡ç®—ä¸¤ä¸ªæ•°å­—çš„å’Œ',
				schema: calculatorSchema
			}
		)

		// 2. è·å–æ¨¡å‹å®ä¾‹å¹¶ç»‘å®šå·¥å…·
		const model = this.getChatModel(provider)
		const modelWithTools = model.bindTools([calculatorTool])

		try {
			// 3. å‘é€ä¸€ä¸ªéœ€è¦ä½¿ç”¨å·¥å…·çš„é—®é¢˜
			const result = await modelWithTools.invoke(
				'è¯·å¸®æˆ‘è®¡ç®— 100 åŠ  200 ç­‰äºå¤šå°‘ï¼Ÿ'
			)

			// 4. æ£€æŸ¥ç»“æœ
			if (result.tool_calls && result.tool_calls.length > 0) {
				console.log('âœ… æ¨¡å‹æ”¯æŒå·¥å…·è°ƒç”¨ï¼')
				console.log(
					'å·¥å…·è°ƒç”¨è¯¦æƒ…:',
					JSON.stringify(result.tool_calls, null, 2)
				)
				return { supported: true, tool_calls: result.tool_calls }
			} else {
				console.log('âŒ æ¨¡å‹æœªè°ƒç”¨å·¥å…·ï¼Œå¯èƒ½ä¸æ”¯æŒæˆ–æœªè¯†åˆ«æ„å›¾ã€‚')
				return { supported: false, response: result.content }
			}
		} catch (error) {
			console.error('âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯:', error)
			return { supported: false, error }
		}
	}
}
